{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e527071d",
   "metadata": {},
   "source": [
    "# DeepAR Only\n",
    "\n",
    "This is part of a modified version of \"Chris Tegho 220111.ipynb\".\n",
    "\n",
    "https://ts.gluon.ai/ \n",
    "\n",
    "I should also refer to \"Chris Tegho via Slack 220110.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from helpers import *\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.model import deepar\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.evaluation import make_evaluation_predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#This is suppress all warnings in the notebook - turn when happy code works\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899322b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redshift user credentials - set here\n",
    "USER = \n",
    "PASSWORD = \n",
    "\n",
    "FCST_PERIOD = 9   #How many months I want to forecast ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SQLAlchemy engine for Redshift database\n",
    "user = USER\n",
    "password = PASSWORD\n",
    "host=\n",
    "port='5439'\n",
    "dbname='prod'\n",
    "\n",
    "url = \"postgresql+psycopg2://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, dbname)\n",
    "engine = create_engine(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd178aa",
   "metadata": {},
   "source": [
    "# A. Get data from Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f87598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is all harcoded for the moment\n",
    "#ALSO I have a single database read version of this code that I can use instead of this\n",
    "#The df_catalog concept was from when I wanted to downselect based on product attributes\n",
    "#e.g. the Business clustering rules\n",
    "\n",
    "query = f\"\"\"\n",
    "select\n",
    "    isbn + ship_to_country_key as key,\n",
    "    isbn,\n",
    "    isbn_short,\n",
    "    subject_2_key,\n",
    "    series_key,\n",
    "    series_short,\n",
    "    family_key,\n",
    "    family_name,\n",
    "    ship_to_country_key as country,\n",
    "    sum(quantity_demanded) as qty_12m\n",
    "from r2ibp.f_demand_actual t1\n",
    "left join r2ibp.lu_product t2\n",
    "on t1.isbn = t2.isbn13\n",
    "where last_day(date) <= current_date\n",
    "and last_day(date) > dateadd(month, -{FCST_PERIOD}, current_date)\n",
    "and ship_to_country_key = 'ES'\n",
    "group by isbn, isbn_short, subject_2_key, series_key, series_short, family_key, family_name, ship_to_country_key\n",
    "order by qty_12m desc\n",
    "\"\"\"\n",
    "\n",
    "conn = engine.connect()\n",
    "df_catalog = pd.read_sql_query(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79614f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(df_catalog['key'])\n",
    "   \n",
    "df_demand = get_demand(key_list, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06fd0c",
   "metadata": {},
   "source": [
    "# B. Pivot into a datafame\n",
    "\n",
    "NB Drop negative values and replace NaNs (i.e. missing values) with zeroes\n",
    "\n",
    "Also simplify the columns index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I only need three columns from df_demand\n",
    "df_temp = df_demand[['key', 'month', 'qty']]\n",
    "\n",
    "df_pivoted = df_temp[df_temp['qty']>0].pivot(index='key', columns='month').fillna(0)\n",
    "df_pivoted.columns = df_pivoted.columns.droplevel(0)\n",
    "\n",
    "del df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebf357",
   "metadata": {},
   "source": [
    "# C. Prepare the data for modelling\n",
    "\n",
    "The prediction will be for FCST_PERIOD months\n",
    "\n",
    "\n",
    "Scaled/ normalise the data - start by just scaling based on max value like I did for clustering\n",
    "\n",
    "Need to split the data both into X and y (12 months) and train (up a year ago) and test (up to the last full month). As well as creating a validation set for the training performance from the train set.\n",
    "\n",
    "Finally convert dataframes into numpy arrays to input into DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc686d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data 0-1 based on the max demand value\n",
    "dfMax = df_pivoted.max(axis=1)\n",
    "df_scaled = df_pivoted.divide(dfMax, axis=0)\n",
    "df_scaled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set key parameters for data prep and modelling\n",
    "n_features = 1 #i.e. a single quantity for each month\n",
    "\n",
    "n_total_steps = df_scaled.shape[1] #i.e. the total number of months\n",
    "n_steps_out = FCST_PERIOD\n",
    "n_steps_in = n_total_steps - 2*n_steps_out # Need to chop off both the train and test FCST_PERIOD months!\n",
    "\n",
    "#Split into train and test X and y\n",
    "df_X = df_scaled.iloc[:, :-(2*n_steps_out)]\n",
    "df_y = df_scaled.iloc[:, -(2*n_steps_out):-n_steps_out]\n",
    "\n",
    "df_X_train, df_X_val, df_y_train, df_y_val = train_test_split(df_X, df_y, train_size=0.92) #default is 75:25 split\n",
    "\n",
    "df_X_test = df_scaled.iloc[:, n_steps_out:-n_steps_out]\n",
    "df_y_test = df_scaled.iloc[:, -n_steps_out:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c31e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "#Format data to be used by DeepAR using ListDataSet()\n",
    "\n",
    "train_ds_list = []\n",
    "for ind_ in df_X_train.index:\n",
    "    target = df_X_train.loc[ind_].to_list() \n",
    "    target +=df_y_train.loc[ind_].to_list()\n",
    "    start = pd.Timestamp(df_X_train.columns[0], freq='M')\n",
    "    train_ds_list += [{\n",
    "            FieldName.TARGET: target, \n",
    "             FieldName.START: start,\n",
    "        }]\n",
    "train_ds = ListDataset(train_ds_list,freq='M')\n",
    "\n",
    "test_ds_list = []\n",
    "for ind_ in df_X_test.index:\n",
    "    target = df_X_test.loc[ind_].to_list() \n",
    "    target +=df_y_test.loc[ind_].to_list()\n",
    "    start = pd.Timestamp(df_X_train.columns[0], freq='M')\n",
    "    test_ds_list += [{\n",
    "            FieldName.TARGET: target, \n",
    "             FieldName.START: start,\n",
    "        }]\n",
    "test_ds = ListDataset(test_ds_list,freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "#This is just to take a look at what's in train_ds\n",
    "\n",
    "#Create an iterable object\n",
    "it = iter(train_ds)\n",
    "#Get the first entry in it\n",
    "train_entry = next(it)\n",
    "\n",
    "#Now convert\n",
    "train_series = to_pandas(train_entry) #This is a series object without headings\n",
    "\n",
    "train_series.head()\n",
    "#I could plot\n",
    "# train_series.plot();\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe8362",
   "metadata": {},
   "source": [
    "# D. Model and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a268a",
   "metadata": {},
   "source": [
    "## D.1 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57500f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(epochs=30, learning_rate=1e-2)\n",
    "estimator = deepar.DeepAREstimator(\n",
    "    freq=\"M\", prediction_length=FCST_PERIOD, trainer=trainer)\n",
    "predictor = estimator.train(training_data=train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e2bea",
   "metadata": {},
   "source": [
    "## D.2 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93808ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_ds,  # test dataset\n",
    "    predictor=predictor,  # predictor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49f0cc",
   "metadata": {},
   "source": [
    "Help on how to analyse the output can be found in https://stackoverflow.com/questions/61416951/export-multiple-gluonts-forecasts-to-pandas-dataframe\n",
    "\n",
    "The code below reformats the forecast_it to the same format as used in the KERAS models for simple comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "def create_sample_df(forecast):\n",
    "    samples = forecast.samples\n",
    "    ns, h = samples.shape\n",
    "    dates = pd.date_range(forecast.start_date, freq=forecast.freq, periods=h)\n",
    "    return pd.DataFrame(samples.T, index=dates)\n",
    "\n",
    "#Iterate forecast_it\n",
    "parts = [create_sample_df(entry).assign(entry=i)\n",
    "         for i, entry in enumerate(forecast_it)]\n",
    "df_temp = pd.concat(parts)\n",
    "\n",
    "#I only want the median value (all 100 percentiles are available in samples)\n",
    "df_temp = df_temp[['entry', 50]].reset_index() \n",
    "#Now pivot to create same structure as the KERAS forecasts (to reuse code)\n",
    "df_temp = df_temp.pivot(index= 'entry', columns = 'index')\n",
    "yhat = df_temp.to_numpy()\n",
    "\n",
    "#Set up the rescaled df for plotting at the same time\n",
    "df_yhat = pd.DataFrame(data=yhat, index=df_y_test.index, columns=df_y_test.columns).mul(dfMax, axis = 0)\n",
    "\n",
    "del df_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe682e",
   "metadata": {},
   "source": [
    "# E. Calculate metrics and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b30e5c",
   "metadata": {},
   "source": [
    "## E.1 Calculate metrics\n",
    "\n",
    "Calculate total demand and RMSE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d289e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function works across the whole arrays\n",
    "\n",
    "def calc_prediction_metrics_from_array(y_test, yhat, y_naive1):\n",
    "    \n",
    "    sum_pred = np.sum(yhat, axis=1)\n",
    "    sum_naive1 = np.sum(y_naive1, axis=1)\n",
    "    sum_act = np.sum(y_test, axis=1)\n",
    "    \n",
    "    diff_pred_act = sum_pred - sum_act\n",
    "    diff_naive1_act = sum_naive1 - sum_act\n",
    "    \n",
    "    abs_pred_closer = (abs(diff_pred_act) < abs(diff_naive1_act))\n",
    "    \n",
    "    rmse_pred = mean_squared_error(y_test.T, yhat.T, multioutput='raw_values', squared = False)\n",
    "    rmse_naive1 = mean_squared_error(y_test.T, y_naive1.T, multioutput='raw_values', squared = False)\n",
    "    \n",
    "    pred_rmse_lower = (rmse_pred < rmse_naive1)\n",
    "    rmse_pc_diff = ((rmse_pred - rmse_naive1)/rmse_naive1)*100\n",
    "\n",
    "    return [sum_naive1, sum_pred, sum_act, diff_naive1_act, diff_pred_act, abs_pred_closer,\n",
    "                                rmse_naive1, rmse_pred, pred_rmse_lower, rmse_pc_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc959df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_X_test.to_numpy()\n",
    "y_test = df_y_test.to_numpy()\n",
    "\n",
    "if FCST_PERIOD == 12:\n",
    "    y_naive1 = X_test[:, -12:] #i.e. 12 months ago\n",
    "else:    \n",
    "    y_naive1 = X_test[:, -12:FCST_PERIOD-12] #i.e. back 12 months and then PERIOD forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "#This suppresses all warnings - in this case divide by zero\n",
    "    \n",
    "metrics = calc_prediction_metrics_from_array(y_test, yhat, y_naive1)\n",
    "\n",
    "df_metrics = pd.DataFrame(df_X_test.index ,columns = ['key'])\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    df_metrics[i] = metrics[i]\n",
    "\n",
    "df_metrics.columns = ['key', 'sum_naive1', 'sum_pred', 'sum_act', 'diff_naive1_act', 'diff_pred_act','abs_pred_closer',\n",
    "                      'rmse_naive1', 'rmse_pred', 'pred_rmse_lower', 'rmse_pc_diff']\n",
    "\n",
    "#Round all values to 2 dp\n",
    "df_metrics = df_metrics.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bb138",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_naive1(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71aef5",
   "metadata": {},
   "source": [
    "## E.2 Plot selected ISBN countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d183896",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_list = ['9780521148597ES', '9780521148559ES', '9781108457651ES', '9781108794091ES', '9781108381208ES',\n",
    "             '9788490365809ES', '9788490369883ES', '9788490361078ES', '9788490369975ES', '9780521221689ES']\n",
    "\n",
    "print(plot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up grid for plotting\n",
    "rows = int(np.ceil(len(plot_list)/2))  #round up\n",
    "fig, axes = plt.subplots(rows, 2, figsize = (16,rows*4))\n",
    "#The following is to iterate the axes\n",
    "axes_flat = axes.flat\n",
    "\n",
    "#Needed to get the period of pred (month values)\n",
    "x_pred = df_y_test.columns\n",
    "\n",
    "for i, key in enumerate(plot_list):\n",
    "    \n",
    "    #Now plotting the full values\n",
    "    actuals = df_pivoted[df_pivoted.index == key]\n",
    "    #convert actuals to ts\n",
    "    ts_actuals = pd.melt(actuals, var_name='month', value_name='qty')\n",
    "    ts_actuals = ts_actuals.set_index('month')\n",
    "    ts_actuals.index = pd.to_datetime(ts_actuals.index)\n",
    "    \n",
    "    #do the same for the predictions\n",
    "    pred = df_yhat[df_yhat.index == key]\n",
    "    ts_pred = pd.melt(pred, var_name='month', value_name='qty')\n",
    "    ts_pred = ts_pred.set_index('month')\n",
    "    ts_pred.index = pd.to_datetime(ts_pred.index)\n",
    "  \n",
    "    #and naive-1\n",
    "    ts_naive1 = ts_actuals[-(12+FCST_PERIOD):-12].shift(periods = 12, freq = 'M')\n",
    "    \n",
    "    ax = axes_flat[i]\n",
    "    ax.plot(ts_actuals[-24:], '-o', label=\"actuals\") #Just the last 2 years\n",
    "    ax.plot(ts_pred, '-o', label=\"predicted\")\n",
    "    ax.plot(ts_naive1, '-o', label=\"naive-1\")\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_title(key);\n",
    "       \n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "df_metrics[df_metrics['key'].isin(plot_list)]\n",
    "\n",
    "#These are all significantly worse than naive-1\n",
    "#NB It doesn't help that we are now forecasting over a quiet part of the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1699dc",
   "metadata": {},
   "source": [
    "* Imbalance - try more balanced training examples (select them manually if possible) - check distribution of data\n",
    "* Take more conventional ML - CatBoost - lightGBM\n",
    "* Predict median of next 12 months - try to figure out conv ML makes sense\n",
    "* Standardisation instead of normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2af0f",
   "metadata": {},
   "source": [
    "# Extra CT Code\n",
    "\n",
    "This is some extra plotting code that I don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prob_forecasts(ts_entry, forecast_entry):\n",
    "    plot_length = 12*4\n",
    "    prediction_intervals = (50.0, 90.0)\n",
    "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
    "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.legend(legend, loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)\n",
    "\n",
    "gh=20 #Let's take element 20\n",
    "forecast_entry = forecasts[gh]\n",
    "ts_entry = tss[gh]\n",
    "\n",
    "plot_prob_forecasts(ts_entry, forecast_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d17c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
