{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5151a386",
   "metadata": {},
   "source": [
    "# Autoclustering\n",
    "\n",
    "Are there any patterns/ groupings in the data that we are missing?\n",
    "\n",
    "NB tslearn needs numpy <= 1.21 (currentt verision is 1.23.2). Hence I need to run in a separate environment\n",
    "\n",
    "Sources for this are:\n",
    "\n",
    "https://www.kaggle.com/izzettunc/introduction-to-time-series-clustering\n",
    "and tslearn documentation i.e. https://tslearn.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from helpers import *\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import datetime as dt\n",
    "import dateutil\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.clustering import KernelKMeans\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#This is suppress all warnings in the notebook - turn on when happy code works\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redshift user credentials - set here\n",
    "USER = \n",
    "PASSWORD = \n",
    "\n",
    "FCST_PERIOD = 9   #How many months I want to forecast ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cf851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SQLAlchemy engine for Redshift database\n",
    "user = USER\n",
    "password = PASSWORD\n",
    "host= \n",
    "port='5439'\n",
    "dbname='prod'\n",
    "\n",
    "url = \"postgresql+psycopg2://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, dbname)\n",
    "engine = create_engine(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd178aa",
   "metadata": {},
   "source": [
    "# A. Get data from Redshift\n",
    "\n",
    "Let's focus on Spain\n",
    "\n",
    "And select everything where there was some demand in the preiod that we want to forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f87598",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DEMAND = 0\n",
    "\n",
    "query = f\"\"\"\n",
    "select\n",
    "    isbn + ship_to_country_key as key,\n",
    "    last_day(date) as month,\n",
    "    sum(quantity_demanded) as qty\n",
    "from r2ibp.f_demand_actual\n",
    "where ship_to_country_key = 'ES'\n",
    "and month <= current_date\n",
    "and key in\n",
    "(\n",
    "select key\n",
    "from\n",
    "(\n",
    "select\n",
    "     isbn + ship_to_country_key as key,\n",
    "     sum(quantity_demanded) as qty_last_12m\n",
    "from r2ibp.f_demand_actual\n",
    "where ship_to_country_key = 'ES'\n",
    "and last_day(date) <= current_date\n",
    "and last_day(date) > dateadd(month, -{FCST_PERIOD}, current_date)\n",
    "and isbn not like '555%%'\n",
    "group by key\n",
    ")\n",
    "where qty_last_12m > {MIN_DEMAND}\n",
    ")\n",
    "group by key, month\n",
    "order by key, month asc\n",
    "\n",
    "\"\"\"\n",
    "conn = engine.connect()\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06fd0c",
   "metadata": {},
   "source": [
    "# B. Pivot into a datafame\n",
    "\n",
    "NB Drop negative values and replace NaNs (i.e. missing values) with zeroes\n",
    "\n",
    "Also simplify the columns index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e058282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted = df[df['qty']>0].pivot(index='key', columns='month').fillna(0)\n",
    "df_pivoted.columns = df_pivoted.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebf357",
   "metadata": {},
   "source": [
    "# C. Prepare the data for modelling\n",
    "\n",
    "I will make use of tslearn's TimeSeriesScalerMinMax scaler and scale between 0 and 1 NB I want to retain the zero values where these exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a870d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMinMax()\n",
    "\n",
    "X = df_pivoted.to_numpy()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#Reshape for plotting later\n",
    "print('Original shape of X', X.shape)\n",
    "\n",
    "num_ts = X.shape[0]\n",
    "X = np.reshape(X, (num_ts, -1))\n",
    "\n",
    "print('Reshaped X', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a454d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sample to speed up calculatinbg silhouette scores\n",
    "\n",
    "df_sample = df_pivoted.sample(frac=0.1, random_state = 1234)\n",
    "\n",
    "X_sample = df_sample.to_numpy()\n",
    "X_sample = scaler.fit_transform(X_sample)\n",
    "\n",
    "num_ts = X_sample.shape[0]\n",
    "X_sample = np.reshape(X_sample, (num_ts, -1))\n",
    "\n",
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab61b6c",
   "metadata": {},
   "source": [
    "# D. Determining the number of clusters\n",
    "\n",
    "Use the Silhouette Score or Elbow plot to determine the best number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1f888",
   "metadata": {},
   "source": [
    "## D.1 Silhouette Scores\n",
    "\n",
    "The full set is going to take forever so use X_sample\n",
    "\n",
    "NB. Small number of clusters take the longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to track progress\n",
    "start = dt.datetime.now()\n",
    "\n",
    "from tslearn.clustering import silhouette_score\n",
    "\n",
    "scores = []\n",
    "\n",
    "num_clusters_list = range(2, 28)\n",
    "\n",
    "for n_clusters in num_clusters_list:\n",
    "    \n",
    "    kmeans = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\")\n",
    "#    kmeans = KernelKMeans(n_clusters=n_clusters)\n",
    "    cluster_assignment = kmeans.fit_predict(X_sample)\n",
    "    scores.append(silhouette_score(X_sample, cluster_assignment))\n",
    "    \n",
    "    print('Calc complete for silhouette score for', n_clusters, 'clusters after', dt.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_clusters_list, scores)\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('K-means Clustering')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(num_clusters_list, scores))\n",
    "\n",
    "#Lowest score at 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc94a8",
   "metadata": {},
   "source": [
    "## D.2 Elbow Plots\n",
    "\n",
    "Same as above i.e. using X_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to calculate the within cluster variance\n",
    "\n",
    "def cluster_variance(points):\n",
    "    \n",
    "    N = points.shape[0]\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            total += 0.5 * np.linalg.norm(points[i,:] - points[j,:]) ** 2\n",
    "            \n",
    "    return total / N**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.datetime.now()\n",
    "\n",
    "all_assignments = []\n",
    "\n",
    "for i in range(1,28):\n",
    "    kmeans = TimeSeriesKMeans(n_clusters=i, metric=\"dtw\")\n",
    "    all_assignments.append(kmeans.fit_predict(X_sample))\n",
    "\n",
    "awcv = []\n",
    "\n",
    "for assignment in all_assignments:\n",
    "    wcv = 0\n",
    "    C = np.max(assignment) + 1\n",
    "    for i in range(C):\n",
    "        wcv += cluster_variance(X_sample[assignment == i])\n",
    "    awcv.append(wcv / C)\n",
    "\n",
    "    print('Calc complete for average within cluster variance for', i+1, 'clusters after', dt.datetime.now() - start)\n",
    "    \n",
    "#plt.plot(range(1,C+1), awcv)\n",
    "plt.plot(range(1,C+1), awcv)\n",
    "\n",
    "plt.xlabel('Number of flat clusters')\n",
    "plt.ylabel('Average within-class variance')\n",
    "plt.title('K-means Clustering')\n",
    "plt.show()\n",
    "\n",
    "#There's no strong elbow.\n",
    "#Presumably the decrease in awcv is simply a matter of the cluster size getting bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(range(1,28),awcv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6914f5",
   "metadata": {},
   "source": [
    "# E Clustering with KMeans\n",
    "\n",
    "Cluster using TS version of K means on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf81f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = math.ceil(math.sqrt(len(X))) \n",
    "# A good rule of thumb is choosing k as the square root of the number of points in the training data set in kNN\n",
    "print(cluster_count)\n",
    "\n",
    "#I'm going to override (as otherwise it will take forever)\n",
    "cluster_count = 16  #This is based on info from later\n",
    "print(cluster_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.datetime.now()\n",
    "print(start)\n",
    "\n",
    "km = TimeSeriesKMeans(n_clusters=cluster_count, metric=\"dtw\")\n",
    "#km = KernelKMeans(n_clusters=cluster_count)\n",
    "\n",
    "labels = km.fit_predict(X)\n",
    "\n",
    "print(dt.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698399e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_unique, n = np.unique(labels, return_counts=True)\n",
    "plt.bar(ar_unique, n);\n",
    "\n",
    "list(zip(ar_unique, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is with DBA\n",
    "\n",
    "plot_count = math.ceil(math.sqrt(cluster_count))\n",
    "\n",
    "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
    "fig.suptitle('Clusters')\n",
    "row_i=0\n",
    "column_j=0\n",
    "for label in set(labels):\n",
    "    cluster = []\n",
    "    for i in range(len(labels)):\n",
    "            if(labels[i]==label):\n",
    "                axs[row_i, column_j].plot(X[i],c=\"gray\",alpha=0.4)\n",
    "                cluster.append(X[i])\n",
    "    if len(cluster) > 0:\n",
    "        axs[row_i, column_j].plot(dtw_barycenter_averaging(np.vstack(cluster)),c=\"red\")\n",
    "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*plot_count+column_j))\n",
    "    column_j+=1\n",
    "    if column_j%plot_count == 0:\n",
    "        row_i+=1\n",
    "        column_j=0\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "# This is definitely more useful than averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a09a2",
   "metadata": {},
   "source": [
    "# F Forecast HW by Cluster\n",
    "\n",
    "Has the autoclustering helped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the cluster labels to df_sample dataframe\n",
    "\n",
    "df_clustered = df_pivoted.copy()\n",
    "df_clustered['cluster'] = list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69494c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#220809 This is not the most sensible way to do this.\n",
    "#Run HWES ion everthing and then add the clusetr labels to df_metrics\n",
    "\n",
    "percent_HWES_rmse_better_list = []\n",
    "\n",
    "for cluster in range(0, cluster_count):\n",
    "#for cluster in range(0, 2):\n",
    "    \n",
    "    key_list = df_clustered[df_clustered['cluster'] == cluster].index.to_list()\n",
    "    \n",
    "    df_demand = get_demand(key_list, engine)\n",
    "    df_errors, df_hwes_forecasts = predict_using_hwes(df_demand, FCST_PERIOD)\n",
    "    df_metrics = calc_prediction_metrics(df_hwes_forecasts)\n",
    "    \n",
    "    total_fcsts = len(df_metrics)\n",
    "    num_hwes_rmse_better = df_metrics['pred_rmse_lower'].sum()\n",
    "    percent_HWES_rmse_better = round((num_hwes_rmse_better/total_fcsts)*100, 1)\n",
    "    \n",
    "    percent_HWES_rmse_better_list.append(percent_HWES_rmse_better)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which clusters perform best\n",
    "list(zip(range(0, cluster_count),n, percent_HWES_rmse_better_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfec8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results['cluster'] = ar_unique\n",
    "df_results['timeseries count'] = n\n",
    "df_results['% HWES RMSE lower'] = percent_HWES_rmse_better_list\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc626a03",
   "metadata": {},
   "source": [
    "# G Time Series Plots\n",
    "\n",
    "Let's look at the best preforming cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot cluster 5 NB only 50 timeseries\n",
    "\n",
    "plot_list = df_clustered[df_clustered['cluster'] == 5].index.to_list()[:10]\n",
    "\n",
    "df_demand = get_demand(plot_list, engine)\n",
    "df_errors, df_hwes_forecasts = predict_using_hwes(df_demand, FCST_PERIOD)\n",
    "df_metrics = calc_prediction_metrics(df_hwes_forecasts)\n",
    "\n",
    "plot_sample_preds(plot_list, df_demand, df_hwes_forecasts, FCST_PERIOD)\n",
    "\n",
    "df_metrics[df_metrics['key'].isin(plot_list)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
