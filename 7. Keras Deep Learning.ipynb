{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13c6fb7",
   "metadata": {},
   "source": [
    "# Keras Deep Learning\n",
    "\n",
    "This is the rig for all the Deep Learning methods that use Keras\n",
    "\n",
    "The code in this notebook draws heavily on Brownlee and therefore looks at:\n",
    "\n",
    "1. MLP\n",
    "2. 1D-CNN\n",
    "3. Stacked LSTM\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from helpers import *\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#This is suppress all warnings in the notebook - turn when happy code works\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redshift user credentials - set here\n",
    "USER = \n",
    "PASSWORD = \n",
    "\n",
    "FCST_PERIOD = 9   #How many months I want to forecast ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SQLAlchemy engine for Redshift database\n",
    "user = USER\n",
    "password = PASSWORD\n",
    "host=\n",
    "port='5439'\n",
    "dbname='prod'\n",
    "\n",
    "url = \"postgresql+psycopg2://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, dbname)\n",
    "engine = create_engine(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd178aa",
   "metadata": {},
   "source": [
    "## 1. Get the catalog of ISBN/countries\n",
    "\n",
    "Hardcoded to Spain with demand in the year preceding the 9 months we want to forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f87598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is all hardcoded for the moment\n",
    "#NB There's a lot of unnecessary stuff here. I'll want to rationalise when I tidy up all the notebooks\n",
    "#NB This two stage approach is the one that I will want to use throughout\n",
    "\n",
    "query = f\"\"\"\n",
    "select\n",
    "    isbn + ship_to_country_key as key,\n",
    "    isbn,\n",
    "    isbn_short,\n",
    "    subject_2_key,\n",
    "    series_key,\n",
    "    series_short,\n",
    "    family_key,\n",
    "    family_name,\n",
    "    ship_to_country_key as country,\n",
    "    sum(quantity_demanded) as qty_12m\n",
    "from r2ibp.f_demand_actual t1\n",
    "left join r2ibp.lu_product t2\n",
    "on t1.isbn = t2.isbn13\n",
    "where last_day(date) <= current_date\n",
    "and last_day(date) > dateadd(month, -{FCST_PERIOD}, current_date)\n",
    "and ship_to_country_key = 'ES'\n",
    "group by isbn, isbn_short, subject_2_key, series_key, series_short, family_key, family_name, ship_to_country_key\n",
    "order by qty_12m desc\n",
    "\"\"\"\n",
    "\n",
    "conn = engine.connect()\n",
    "df_catalog = pd.read_sql_query(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c7b88",
   "metadata": {},
   "source": [
    "## 2. Get demand data for the test cases\n",
    "\n",
    "Read all the demand data for the selected ISBN/countries. In this case Spanish ISBNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a904ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(df_catalog['key'])\n",
    "   \n",
    "df_demand = get_demand(key_list, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06fd0c",
   "metadata": {},
   "source": [
    "## 3. Pivot into a datafame\n",
    "\n",
    "NB Drop negative values and replace NaNs (i.e. missing values) with zeroes\n",
    "\n",
    "Also simplify the columns index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e058282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I only need three columns from df_demand\n",
    "df_temp = df_demand[['key', 'month', 'qty']]\n",
    "\n",
    "df_pivoted = df_temp[df_temp['qty']>0].pivot(index='key', columns='month').fillna(0)\n",
    "df_pivoted.columns = df_pivoted.columns.droplevel(0)\n",
    "\n",
    "del df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A selection of data to look at\n",
    "df_pivoted.iloc[-10:, 22:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebf357",
   "metadata": {},
   "source": [
    "## 4. Prepare the data for modelling\n",
    "\n",
    "The prediction will be for the last 9 months (set by PERIOD)\n",
    "\n",
    "Scaled/ normalise the data - start by just scaling based on max value\n",
    "\n",
    "Need to split the data both into X and y and train and test. As well as creating a validation set for the training performance from the train set.\n",
    "\n",
    "Finally convert dataframes into numpy arrays to input into keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc686d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data 0-1 based on the max demand value\n",
    "dfMax = df_pivoted.max(axis=1)\n",
    "df_scaled = df_pivoted.divide(dfMax, axis=0)\n",
    "df_scaled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set key parameters for data prep and modelling\n",
    "n_features = 1 #i.e. a single quantity for each month\n",
    "\n",
    "n_total_steps = df_scaled.shape[1] #i.e. the total number of months\n",
    "n_steps_out = FCST_PERIOD\n",
    "n_steps_in = n_total_steps - 2*n_steps_out # Need to chop off both the train and test 9 months!\n",
    "\n",
    "#Split into train and test X and y\n",
    "df_X = df_scaled.iloc[:, :-(2*n_steps_out)]\n",
    "df_y = df_scaled.iloc[:, -(2*n_steps_out):-n_steps_out]\n",
    "\n",
    "df_X_train, df_X_val, df_y_train, df_y_val = train_test_split(df_X, df_y) #default is 75:25 split\n",
    "\n",
    "df_X_test = df_scaled.iloc[:, n_steps_out:-n_steps_out] #X_test needs to be the same length as X_train\n",
    "df_y_test = df_scaled.iloc[:, -n_steps_out:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2eed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dfs to numpy arrays\n",
    "X_train = df_X_train.to_numpy()\n",
    "X_val = df_X_val.to_numpy()\n",
    "X_test = df_X_test.to_numpy()\n",
    "y_train = df_y_train.to_numpy()\n",
    "y_val = df_y_val.to_numpy()\n",
    "y_test = df_y_test.to_numpy()\n",
    "\n",
    "#Create y_naive from X_test\n",
    "y_naive = y_naive1 = X_test[:, -12:FCST_PERIOD-12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe8362",
   "metadata": {},
   "source": [
    "## 5. Model and Predict\n",
    "\n",
    "Allow myself the option of running various models here.\n",
    "These models are taken from the Brownlee pdf book\n",
    "\n",
    "NB I also should run each of these models multiple times to see what variance there is in the results.\n",
    "Compare with how Brownlee did this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ddbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_MLP():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim = n_steps_in))\n",
    "    model.add(Dense(n_steps_out, activation='relu')) #This is to zero the negative values\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_CNN():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(n_steps_out, activation='relu'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LSTM():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "    model.add(LSTM(100, activation='relu'))\n",
    "    model.add(Dense(n_steps_out, activation = 'relu'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3239f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose model to run here\n",
    "#There also needs to be some data reformating\n",
    "MODEL = 'LSTM'\n",
    "\n",
    "if MODEL == 'MLP':  \n",
    "    model = define_MLP()\n",
    "else:\n",
    "    # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], n_features))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "\n",
    "    if MODEL == 'CNN':\n",
    "        model = define_CNN()\n",
    "    elif MODEL == 'LSTM':\n",
    "        model = define_LSTM()\n",
    "    else:\n",
    "        model = define_LSTM()\n",
    "                            \n",
    "#compile model - do this every time to reset the weights\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#display what the model looks like\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.datetime.now()\n",
    "\n",
    "# fit model\n",
    "history = model.fit(X_train, y_train, batch_size = 128, epochs=30, validation_data=(X_val, y_val), verbose=2)\n",
    "#2000 epochs in Brownlee\n",
    "\n",
    "time = dt.datetime.now() - start\n",
    "print('Time to fit model', time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb209ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation set results:')\n",
    "model.evaluate(X_val, y_val)\n",
    "\n",
    "print('Test set results:')\n",
    "model.evaluate(X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict for the test set\n",
    "yhat = model.predict(X_test, verbose=0)\n",
    "\n",
    "#And convert to dataframe for later\n",
    "df_yhat = pd.DataFrame(data=yhat, index=df_y_test.index, columns=df_y_test.columns)\n",
    "#I'll rescale this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yhat.describe()\n",
    "#Never calcs full value either (it's never less than zero due to use of Relu in the output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa39222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_test.describe()\n",
    "#Notice how the yhat values are significantly lower than the actuals (shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yhat = df_yhat.mul(dfMax, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe682e",
   "metadata": {},
   "source": [
    "## 6. Calculate metrics and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b30e5c",
   "metadata": {},
   "source": [
    "### Calculate metrics\n",
    "\n",
    "Calculate total demand and RMSE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function works across the whole arrays\n",
    "\n",
    "def calc_prediction_metrics_from_array(y_test, yhat, y_naive1):\n",
    "    \n",
    "    sum_pred = np.sum(yhat, axis=1)\n",
    "    sum_naive1 = np.sum(y_naive1, axis=1)\n",
    "    sum_act = np.sum(y_test, axis=1)\n",
    "    \n",
    "    diff_pred_act = sum_pred - sum_act\n",
    "    diff_naive1_act = sum_naive1 - sum_act\n",
    "    \n",
    "    abs_pred_closer = (abs(diff_pred_act) < abs(diff_naive1_act))\n",
    "    \n",
    "    rmse_pred = mean_squared_error(y_test.T, yhat.T, multioutput='raw_values', squared = False)\n",
    "    rmse_naive1 = mean_squared_error(y_test.T, y_naive1.T, multioutput='raw_values', squared = False)\n",
    "    \n",
    "    pred_rmse_lower = (rmse_pred < rmse_naive1)\n",
    "    rmse_pc_diff = ((rmse_pred - rmse_naive1)/rmse_naive1)*100\n",
    "\n",
    "    return [sum_naive1, sum_pred, sum_act, diff_naive1_act, diff_pred_act, abs_pred_closer,\n",
    "                                rmse_naive1, rmse_pred, pred_rmse_lower, rmse_pc_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "#This suppresses all warnings - in this case divide by zero\n",
    "\n",
    "if MODEL == 'MLP':  \n",
    "    if FCST_PERIOD == 12:\n",
    "        y_naive1 = X_test[:, -12:] #i.e. 12 months ago\n",
    "    else:    \n",
    "        y_naive1 = X_test[:, -12:FCST_PERIOD-12] #i.e. back 12 months and then PERIOD forward\n",
    "\n",
    "else:\n",
    "    if FCST_PERIOD == 12:\n",
    "        y_naive1 = X_test[:, -12:, 0]\n",
    "    else:    \n",
    "        y_naive1 = X_test[:, -12:FCST_PERIOD-12, 0] # different input data structures\n",
    "    \n",
    "metrics = calc_prediction_metrics_from_array(y_test, yhat, y_naive1)\n",
    "\n",
    "df_metrics = pd.DataFrame(df_X_test.index ,columns = ['key'])\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    df_metrics[i] = metrics[i]\n",
    "\n",
    "df_metrics.columns = ['key', 'sum_naive1', 'sum_pred', 'sum_act', 'diff_naive1_act', 'diff_pred_act','abs_pred_closer',\n",
    "                      'rmse_naive1', 'rmse_pred', 'pred_rmse_lower', 'rmse_pc_diff']\n",
    "\n",
    "#Round all values to 2 dp\n",
    "df_metrics = df_metrics.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025fa38e",
   "metadata": {},
   "source": [
    "### Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_naive1(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5007aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_metrics_sorted = df_metrics.sort_values(by='diff_rmses_percent', ascending = True)\n",
    "df_metrics_sorted = df_metrics.sort_values(by='rmse_pc_diff', ascending = True)\n",
    "df_metrics_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71aef5",
   "metadata": {},
   "source": [
    "## E.2 Plot selected ISBN countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d183896",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_list = ['9780521148597ES', '9780521148559ES', '9781108457651ES',\n",
    "             '9781108794091ES', '9781108381208ES', '9788490365809ES',\n",
    "             '9788490369883ES', '9788490361078ES', '9788490369975ES', '9780521221689ES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up grid for plotting\n",
    "rows = int(np.ceil(len(plot_list)/2))  #round up\n",
    "fig, axes = plt.subplots(rows, 2, figsize = (16,rows*4))\n",
    "#The following is to iterate the axes\n",
    "axes_flat = axes.flat\n",
    "\n",
    "#Needed to get the period of pred (month values)\n",
    "x_pred = df_y_test.columns\n",
    "\n",
    "for i, key in enumerate(plot_list):\n",
    "    \n",
    "    actuals = df_pivoted[df_pivoted.index == key]\n",
    "    #convert actuals to ts\n",
    "    ts_actuals = pd.melt(actuals, var_name='month', value_name='qty')\n",
    "    ts_actuals = ts_actuals.set_index('month')\n",
    "    ts_actuals.index = pd.to_datetime(ts_actuals.index)\n",
    "    \n",
    "    #do the same for the predictions\n",
    "    pred = df_yhat[df_yhat.index == key]\n",
    "    ts_pred = pd.melt(pred, var_name='month', value_name='qty')\n",
    "    ts_pred = ts_pred.set_index('month')\n",
    "    ts_pred.index = pd.to_datetime(ts_pred.index)\n",
    "  \n",
    "    #and naive-1\n",
    "    ts_naive1 = ts_actuals[-(12+FCST_PERIOD):-12].shift(periods = 12, freq = 'M')\n",
    "    \n",
    "    ax = axes_flat[i]\n",
    "    ax.plot(ts_actuals[-24:], '-o', label=\"actuals\") #Just the last 2 years\n",
    "    ax.plot(ts_pred, '-o', label=\"predicted\")\n",
    "    ax.plot(ts_naive1, '-o', label=\"naive-1\")\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_title(key);\n",
    "       \n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "df_metrics[df_metrics['key'].isin(plot_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25590b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
